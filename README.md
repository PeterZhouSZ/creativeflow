
![ground truth](shading_styles.jpg)
![styles](http://www.cs.toronto.edu/creativeflow/fig/truth.jpg)

# Creative Flow+ Codebase
Code accompanying the Creative Flow+ Dataset, CVPR 2019. For paper and data downloads see [project website](http://www.cs.toronto.edu/creativeflow/).

This code can be used to:
   1. Decompress Creative Flow+ Dataset files
   2. Install and use python utilities for working with this data 
   3. Run the data generation pipeline to make your own data
   
We detail all 3 use cases below.

### 1 Decompressing Creative Flow+ Dataset

[Creative Flow+ Dataset](http://www.cs.toronto.edu/creativeflow/) is provided as multiple zip files. The data in the zip files is further compressed using different techniques for different data types. This section explains how to unpack this data into raw images, flow files, etc.

##### Unzipping Packages
It is important to unzip all your download packages into the same directory structure. For example, TODO.

##### Running Decompression

We provide a single script for decompressing the data of interest.
```bash
./datagen/pipeline_decompress.sh -h
```
For example, TODO.

##### About Compressed Data
Most renders are provided as *mp4* files. Objectid images are simply zipped for every animated sequence. Flows, back flows, and lossless depth are packed into a special zip (of a flattened numpy array containing all data for a sequence, which yielded much better compression). Depth is also rendered into a normalized video, with max/min range file included alongside. Lossless normals and correspondences are included as uncompressed `mp4 files`. Following decompression, flow is written as Middlebury `.flo` files, most renders as `.png` images and uncompressed depth as serialized `numpy` arrays. Utilities for reading this data, including compressed and decompressed flows, are included in our python library (next section).

### 2 Installing and Using Python Utilities
The bulk of the python code here can only run inside Blender. We only provide the following python modules that can be used independently and installed using `setup.py`:
  * `creativeflow.blender.flow_util`
  * `creativeflow.blender.io_util`
  * `creativeflow.blender.torch_util`

We suggest reading the code for these modules to find useful functions.

### 3 Data Generation Pipeline
We provide python code compatible with Blender 2.79 API, bash scripts and style assets that can be used to generate your own stylized data with ground truth optical flow, depth, normals, correspondences and object labels. *Note that our pipeline also renders the blends in their original style and can be used even if stylization utilities are not needed by selecting stages of interest when configuring the script.*

##### Requirements
1. Install Blender 2.79 (version is important!) and ensure it is available via shell command `blender`. 

2. Install python dependencies (use python3): `pip install -r requirements.pip`. You may want to create a virtualenv and activate it first: `python3 -m venv cfenv; source cfenv/bin/activate`.

3. Some styles generated by our pipeline rely on "Stylit: example-based stylization of 3D renderings", Fi≈°er et al, SIGGRAPH 2016. The binary for the [stylit algorithm](http://stylit.org/) that can run non-interactively from the command line is **not publicly available**. Without it, our code can create only Blender-based styles (see image above). You may be able to obtain a binary for research purposes from the authors of that work, or use a [newer faster version of stylit](https://dcgi.fel.cvut.cz/home/sykorad/styleblit.html). When available, stylit binary should be exported as shell environment variable `STYLIT_BINARY`.

##### Required Input
Our pipeline generates stylized videos with ground truth annotation using the following inputs:
1. Animated blend files.
2. Background images (we use our own test/train split of the [BAM dataset](https://bam-dataset.org/)).
3. Material and line styles saved in a blend file (we provide `creativeflow/assets/train/styles.blend` and  `creativeflow/assets/test/styles.blend`)
4. (optional) Carefully preprocessed hand-drawn style exemplars for stylit (we provide `assets/train/stylit_styles/` and `assets/test/stylit_styles/`)
5. (optional) Crowd-sourced color combinations to make random color assignments more representative of choices human animators would make (we provide `assets/train/colors.txt` and `assets/test/colors.txt`)

Our blender-based python scripts offer some assistance in creating your animated blend files (1). E.g., you may be able to adapt `animate_main.py`, `create_corresp_blends_main.py` and `retarget_main.py` to suit your purpose.

##### Testing Your Set Up
To understand the steps of our pipeline and test local set up, run the regression test:
```bash
cd creativeflow/creativeflow

# See how to set custom test options, e.g. output dir
./tests/pipeline_regression_test.sh -h

# Actually run the test
./tests/pipeline_regression_test.sh
```

##### Data Generation Pipeline
Data generation is a single configurable script, broken down into 16 individual stages. Regression test (above) runs this script using test blends provided and should give a good starting point.
```bash
cd creativeflow/creativeflow

# See pipeline script help
./datagen/pipeline.sh -h

# Run pipeline (see examples in tests/pipeline_regression_test.sh)
./datagen/pipeline.sh <YOUR FLAGS>
```